{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3d8bbc-473d-46dd-8d89-cb3f23c1f2de",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lectures 7: Class demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9babf668-3776-484d-a83f-9b1af3b43dd7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812265e-c771-4be8-9c02-5eae4a42121b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(\"..\"), (\"..\"), \"code\"))\n",
    "\n",
    "from plotting_functions import *\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "DATA_DIR = os.path.join(os.path.abspath(\"..\"), (\"..\"), \"data/\")\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceade0f-bdc9-4989-b140-a5a10d62ad47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demo: Model interpretation of linear classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9861131-b23b-4471-99a7-b4c54652da4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- One of the primary advantage of linear classifiers is their ability to interpret models. \n",
    "- For example, with the sign and magnitude of learned coefficients we could answer questions such as which features are driving the prediction to which direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf88452-bd66-4a36-b1d3-4f04c56291c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We'll demonstrate this by training `LogisticRegression` on the famous [IMDB movie review](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) dataset. The dataset is a bit large for demonstration purposes. So I am going to put a big portion of it in the test split to speed things up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f30540-e09d-4d51-b9d7-8a36bf743162",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = pd.read_csv(DATA_DIR + \"imdb_master.csv\", encoding=\"ISO-8859-1\")\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4d270-7c3f-489c-b88a-ae70957274a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's clean up the data a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e4b670-3087-4426-aa82-a7bbc78e67d4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_tags(doc):\n",
    "    doc = doc.replace(\"<br />\", \" \")\n",
    "    doc = re.sub(r\"https://\\S*\", \"\", doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b452783c-98a4-48f6-9f6d-a8c1a0b38766",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "imdb_df[\"review_pp\"] = imdb_df[\"review\"].apply(replace_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef93373-043d-4a96-a2d6-19d542cccee1",
   "metadata": {},
   "source": [
    "Are we breaking the Golden rule here? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d3353d-0b4a-4019-904e-ccf51ea538fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's split the data and create bag of words representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ffaee-3d50-4ddd-8d3c-26818cc5d13a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(imdb_df, test_size=0.9, random_state=123)\n",
    "X_train, y_train = train_df[\"review_pp\"], train_df[\"sentiment\"]\n",
    "X_test, y_test = test_df[\"review_pp\"], test_df[\"sentiment\"]\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d205ba-8331-4864-b8e4-2fd1a0f41f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer\n",
    "bow = vec.fit_transform(X_train)\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f58c2-06d9-4317-9c9f-53e05f63fefb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examining the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41f6d8-94be-489a-93b0-1e886cbcb0e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The vocabulary (mapping from feature indices to actual words) can be obtained using `get_feature_names_out()` on the `CountVectorizer` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6fe4e-f01e-45b9-aaad-e40c0bf34431",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f816df9-1ea0-4728-9f38-cf3f168c7466",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# first few words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d20bba9-94a2-46ec-8046-87667322a32c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# some middle words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9290f-c4e2-4855-8261-f7c2633a865c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# words with a step of 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ae6ee-90b5-40f2-a537-3e60c42863b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb91777-ffce-4202-885a-82911f13ae77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model building on the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17825ada-4c67-4025-85c8-4a9c41ed591e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First let's try `DummyClassifier` on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf6b7d-a526-4774-afc7-9d6b068126df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier()\n",
    "scores = cross_validate(dummy, X_train, y_train, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b09701-e6a8-49b7-ba2c-be3418d187c4",
   "metadata": {},
   "source": [
    "We have a balanced dataset. \n",
    "\n",
    "**What suggests this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e889b-c90f-4483-9a57-024303200cac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's try logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4940a-ade4-4726-850e-776d03393e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = \n",
    "scores = cross_validate(pipe_lr, X_train, y_train, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada0ee11-b543-4a95-8e20-93f27dc80fc9",
   "metadata": {},
   "source": [
    "Seems like we are overfitting. Let's optimize the hyperparameter `C` of LR and `max_features` of `CountVectorizer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63811c83-a038-4ff2-b2f8-ec12ce50c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {\n",
    "    \"C\": 10.0 ** np.arange(-3, 3, 1),\n",
    "    \"mean_train_scores\": list(),\n",
    "    \"mean_cv_scores\": list(),\n",
    "}\n",
    "for C in scores_dict[\"C\"]:\n",
    "    pipe_lr = make_pipeline(\n",
    "        CountVectorizer(max_features=10000, stop_words=\"english\"),\n",
    "        LogisticRegression\n",
    "    )\n",
    "    scores = cross_validate(pipe_lr, X_train, y_train, return_train_score=True)\n",
    "    scores_dict[\"mean_train_scores\"].append(scores[\"train_score\"].mean())\n",
    "    scores_dict[\"mean_cv_scores\"].append(scores[\"test_score\"].mean())\n",
    "results_df = pd.DataFrame(scores_dict)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6291cb-8d0c-4674-963c-cb2a6f9f2433",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_C = results_df[\"C\"][results_df[\"mean_cv_scores\"].idxmax()]\n",
    "print(\n",
    "    \"The maximum validation score is %0.3f at C = %0.2f \"\n",
    "    % (\n",
    "        np.max(results_df[\"mean_cv_scores\"]),\n",
    "        optimized_C,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d791d-b701-427c-abe1-ade22a8d08a0",
   "metadata": {},
   "source": [
    "Let's finalise the model and use our best hyperparameter value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db0df97-9fd6-428a-9170-6bf5f4e7f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(\n",
    "    CountVectorizer(max_features=10000, stop_words=\"english\"),\n",
    "    LogisticRegression\n",
    ")\n",
    "pipe_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7a8641-7b35-4fe3-b39e-381ea072942a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examining learned coefficients "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f51909-32d4-4f64-946c-2182b006da5b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- The learned coefficients are exposed by the `coef_` attribute of [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00cbdb7-7da2-4472-b5f1-3b7b60ce170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = pipe_lr.named_steps['countvectorizer'].get_feature_names_out().tolist()\n",
    "\n",
    "# Get coefficients \n",
    "coeffs = pipe_lr.named_steps[\"logisticregression\"].coef_.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf51300-765c-49d2-a87c-8fca9410e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_coeff_df = pd.DataFrame\n",
    "word_coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc85fc24-317d-4e67-bbc4-f4bf794cad5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's sort the coefficients in descending order. \n",
    "- Interpretation\n",
    "    - if $w_j > 0$ then increasing $x_{ij}$ moves us toward predicting $+1$. \n",
    "    - if $w_j < 0$ then increasing $x_{ij}$ moves us toward predicting $-1$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff5bcbb-da25-4e14-8ba3-c879a33a07c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0daa02b-da50-429f-a4e2-9256b9ed25bd",
   "metadata": {},
   "source": [
    "- The coefficients make sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696edbf8-6ae8-425c-a5da-d0e627cefb6c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's visualize the top 20 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c277a-acfb-41b9-aa35-5f1bbe1a96ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.tools.visualize_coefficients(coeffs, feature_names, n_top_features=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab454ea-de44-4c17-912a-771fb2359fd1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's explore prediction of the following new review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b90ff4-d3f9-4bb9-9ce0-3479581d5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_reviews = [\n",
    "    \"It got a bit boring at times but the direction was excellent and the acting was flawless. Overall I enjoyed the movie and I highly recommend it!\",\n",
    "    \"The plot was shallower than a kiddie pool in a drought, but hey, at least we now know emojis should stick to texting and avoid the big screen.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3348e587-8cb2-4e4d-a321-d10fde70070c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's get prediction probability scores of the fake review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2087419c-8602-4303-983a-1f7415d0495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr.predict(fake_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c985b9a-52be-4153-a61d-596f4ffb956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities for fake reviews \n",
    "pipe_lr.predict_proba(fake_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392db22-4c70-41d0-accb-a33e7aebbdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ec98e-e607-43fd-837b-34cee994d756",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can find which of the vocabulary words are present in this review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731965cb-66a5-4827-b74e-ec4ff19e817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coeff_example(model, review, coeffs, feature_names, n_top_feats=6):\n",
    "    print(review)\n",
    "    feat_vec = model.named_steps[\"countvectorizer\"].transform([review])\n",
    "    words_in_ex = feat_vec.toarray().ravel().astype(bool)\n",
    "\n",
    "    ex_df = pd.DataFrame(\n",
    "        data=coeffs[words_in_ex],\n",
    "        index=np.array(feature_names)[words_in_ex],\n",
    "        columns=[\"Coefficient\"],\n",
    "    )\n",
    "    mglearn.tools.visualize_coefficients(\n",
    "    coeffs[words_in_ex], np.array(feature_names)[words_in_ex], n_top_features=n_top_feats\n",
    "    )\n",
    "    return ex_df.sort_values(by=[\"Coefficient\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c022f-7adf-4c62-907a-aef04551d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coeff_example(pipe_lr, fake_reviews[0], coeffs, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049fa489-046b-424c-b2b1-22c4c255c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coeff_example(pipe_lr, fake_reviews[1], coeffs, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f50d3ed-b26a-4fb4-ad47-a3058b4f8cb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf58fa9d-81c7-4288-9f1b-339b8f8d0e33",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Most positive review "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a507cd-f97c-4d2e-9178-21b75855d82e",
   "metadata": {},
   "source": [
    "- Remember that you can look at the probabilities (confidence) of the classifier's prediction using the `model.predict_proba` method.\n",
    "- Can we find the reviews where our classifier is most certain or least certain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc71504d-6ef7-4d4d-bf8e-aed651f7d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only get probabilities associated with pos class\n",
    "pos_probs =  # only get probabilities associated with pos class\n",
    "pos_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc44c2-5387-4346-96de-42d9e98620dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What's the index of the example where the classifier is most certain (highest `predict_proba` score for positive)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c44e8-aeae-44db-82c8-ff3db10fb328",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_positive_id = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d065d-5ab3-4b6f-8fe6-c0f19c654202",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"True target: %s\\n\" % (y_train.iloc[most_positive_id]))\n",
    "print(\"Predicted target: %s\\n\" % (pipe_lr.predict(X_train.iloc[[most_positive_id]])[0]))\n",
    "print(\"Prediction probability: %0.4f\" % (pos_probs[most_positive_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3587da-acb1-490a-8d09-cabc6ff2c9ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's examine the features associated with the review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d136b23-c082-4b76-9f00-4890e534248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coeff_example(pipe_lr, X_train.iloc[most_positive_id], coeffs, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767203c4-8837-4b87-9e5c-432e76a4634b",
   "metadata": {},
   "source": [
    "The review has both positive and negative words but the words with **positive** coefficients win in this case! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f00b9-1466-4052-854e-a59c70b414d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Most negative review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49743b5-cae4-4530-aeb1-ff4e8db1e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_probs = # only get probabilities associated with neg class\n",
    "neg_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ffe697-3b4f-4e53-afc0-95439faa31d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_negative_id = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00df177e-ac45-4085-a750-6b60a2ff2d15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Review: %s\\n\" % (X_train.iloc[[most_negative_id]]))\n",
    "print(\"True target: %s\\n\" % (y_train.iloc[most_negative_id]))\n",
    "print(\"Predicted target: %s\\n\" % (pipe_lr.predict(X_train.iloc[[most_negative_id]])[0]))\n",
    "print(\"Prediction probability: %0.4f\" % (neg_probs[most_negative_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df862bd-343c-4f02-9cda-02c70122a8b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_coeff_example(pipe_lr, X_train.iloc[most_negative_id], coeffs, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d0bb45-61ba-4c50-b20e-7e37f06b9e07",
   "metadata": {},
   "source": [
    "The review has both positive and negative words but the words with negative coefficients win in this case! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385256d5-a866-42d5-bf52-07a5556cb598",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14e1d17-1783-410a-a4ed-3cc1c4d54623",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Question for you to ponder on \n",
    "\n",
    "- Is it possible to identify most important features using $k$-NNs? What about decision trees?  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330] *",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
