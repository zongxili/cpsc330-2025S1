{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lecture 15: More Clustering \n",
    "\n",
    "UBC 2024-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture plan and learning outcomes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yellowbrick'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m euclidean_distances\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myellowbrick\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SilhouetteVisualizer\n\u001b[32m     23\u001b[39m plt.rcParams[\u001b[33m\"\u001b[39m\u001b[33mfont.size\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[32m16\u001b[39m\n\u001b[32m     24\u001b[39m plt.rcParams[\u001b[33m\"\u001b[39m\u001b[33mfigure.figsize\u001b[39m\u001b[33m\"\u001b[39m] = (\u001b[32m5\u001b[39m, \u001b[32m4\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'yellowbrick'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(\"..\"), \"code\"))\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotting_functions import *\n",
    "from plotting_functions_unsup import *\n",
    "from scipy.cluster.hierarchy import dendrogram, fcluster, linkage\n",
    "from sklearn import cluster, datasets, metrics\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "plt.rcParams[\"figure.figsize\"] = (5, 4)\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning outcomes\n",
    "\n",
    "From this lecture, students are expected to be able to:\n",
    "\n",
    "- Identify limitations of K-Means.\n",
    "- Broadly explain how DBSCAN works.\n",
    "- Apply DBSCAN using `sklearn`. \n",
    "- Explain the effect of epsilon and minimum samples hyperparameters in DBSCAN.  \n",
    "- Explain the difference between core points, border points, and noise points in the context of DBSCAN. \n",
    "- Identify DBSCAN limitations.\n",
    "- Explain the idea of hierarchical clustering.\n",
    "- Visualize dendrograms using `scipy.cluster.hierarchy.dendrogram`.\n",
    "- Use different truncation levels in dendrogram and flatten clusters using `fcluster`.   \n",
    "- Broadly explain the differences between different linkage criteria. \n",
    "- Explain the advantages and disadvantages of different clustering methods. \n",
    "- Apply clustering algorithms on image datasets and interpret clusters. \n",
    "- Recognize the impact of distance measure and representation in clustering methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.1 Select all of the following statements which are **True** (iClicker)\n",
    "\n",
    "- (A) Similar to K-nearest neighbours, K-Means is a non parametric model.\n",
    "- (B) The meaning of $K$ in K-nearest neighbours and K-Means clustering is very similar. \n",
    "- (C) Scaling of input features is crucial in clustering.  \n",
    "- (D) In clustering, it's almost always a good idea to find equal-sized clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap and motivation [[video](https://youtu.be/1ZwITQyWpkY)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means recap \n",
    "- We discussed K-Means clustering in the previous lecture. \n",
    "- Each cluster is represented by a center. \n",
    "- Given a new point, you can assign it to a cluster by computing the distances to all cluster centers and picking the cluster with the smallest distance. \n",
    "- It's a popular algorithm because \n",
    "    - It's easy to understand and implement.\n",
    "    - Runs relatively quickly and scales well to large datasets. \n",
    "    - `sklearn` has a more scalable variant called [`MiniBatchKMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html) which can handle very large datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means limitations\n",
    "\n",
    "- Relies on random initialization and so the outcome may change depending upon this initialization. \n",
    "- K-Means clustering requires to specify the number of clusters in advance.\n",
    "- Very often you do not know the centers in advance. The elbow method or the silhouette method to find the optimal number of clusters are not always easy to interpret. \n",
    "- Each point has to have a cluster assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means limitations: Shape of K-Means clusters\n",
    "\n",
    "- K-Means partitions the space based on the closest mean. \n",
    "- Each cluster is defined solely by its center and so it can only capture relatively simple shapes. \n",
    "- So the boundaries between clusters are linear; It fails to identify clusters with complex shapes. \n",
    "[Source](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html).\n",
    "\n",
    "![](../img/kmeans_boundaries.png)\n",
    "\n",
    "<!-- <center>\n",
    "<img src=\"img/kmeans_boundaries.png\" alt=\"\" height=\"400\" width=\"400\"> \n",
    "</center>    \n",
    " -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means: failure case 1\n",
    "\n",
    "- K-Means performs poorly if the clusters have more complex shapes (e.g., two moons data below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans(X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means: failure case 2 \n",
    "\n",
    "- Again, K-Means is unable to capture complex cluster shapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datasets.make_circles(n_samples=200, noise=0.06, factor=0.4)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans(X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means: failure case 3\n",
    "\n",
    "- It assumes that all directions are equally important for each cluster and fails to identify non-spherical clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some random cluster data\n",
    "X, y = make_blobs(random_state=170, n_samples=200)\n",
    "rng = np.random.RandomState(74)\n",
    "transformation = rng.normal(size=(2, 2))\n",
    "X = np.dot(X, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans(X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Can we do better than this? \n",
    "- Another clustering algorithm called DBSCAN is able to tackle some of these cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DBSCAN [[video](https://youtu.be/T4NLsrUaRtg)]\n",
    "\n",
    "- **D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DBSCAN introduction\n",
    "\n",
    "- DBSCAN is a density-based clustering algorithm. \n",
    "- Intuitively, it's based on the idea that clusters form dense regions in the data and so it works by identifying \"crowded\" regions in the feature space. \n",
    "- It can address some of the limitations of K-Means we saw above. \n",
    "    - It does not require the user to specify the number of clusters in advance. \n",
    "    - It can identify points that are not part of any clusters. \n",
    "    - It can capture clusters of complex shapes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try `sklearn`'s DBSCAN.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.08, random_state=42)\n",
    "dbscan = DBSCAN(eps=0.2)\n",
    "dbscan.fit(X)\n",
    "plot_original_clustered(X, dbscan, dbscan.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- DBSCAN is able to capture half moons shape\n",
    "- We don't not have to specify the number of clusters. \n",
    "    - That said, it has two other non-trivial hyperparameters to tune. \n",
    "- There are two examples which have not been assigned any label (noise examples). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One more example of DBSCAN clusters capturing complex cluster shapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datasets.make_circles(n_samples=200, noise=0.06, factor=0.4)[0]\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=3)\n",
    "dbscan.fit(X)\n",
    "plot_original_clustered(X, dbscan, dbscan.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does it work?\n",
    "\n",
    "- Iterative algorithm.  \n",
    "- Based on the idea that clusters form dense regions in the data. \n",
    "\n",
    "![](../img/DBSCAN_search.gif)\n",
    "\n",
    "<!-- <center>\n",
    "<img src=\"img/DBSCAN_search.gif\" alt=\"\" height=\"900\" width=\"900\"> \n",
    "</center>\n",
    " -->\n",
    "[Source](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "There are three kinds of points.\n",
    "\n",
    "- **Core points** are the points that have at least `min_samples` points within a distance of `eps`\n",
    "\n",
    "- **Border points** are connected to a core point. They are within a distance of eps to core point but they have fewer than `min_samples` points within a distance of `eps`. \n",
    "\n",
    "- **Noise points** are the points which do not belong to any cluster. In other words, the points which have less than `min_samples` points within distance `eps` of the starting point are noise points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**DBSCAN algorithm**\n",
    "\n",
    "- Pick a point $p$ at random.\n",
    "\n",
    "- Check whether $p$ is a \"core\" point or not. You can check this by looking at the number of neighbours within epsilon distance if they have at least `min_samples` points in the neighbourhood\n",
    "- If $p$ is a core point, give it a colour (label). \n",
    "- Spread the colour of $p$ to all of its neighbours.\n",
    "- Check if any of the neighbours that received the colour is a core point, if yes, spread the colour to its neighbors as well.\n",
    "- Once there are no more core points left to spread the colour, pick a new unlabeled point $p$ and repeat the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBSCAN Analogy**\n",
    "\n",
    "Consider DBSCAN in a social context: \n",
    "- Social butterflies (ü¶ã): Core points\n",
    "- Friends of social butterflies who are not social butterflies: Border points\n",
    "- Lone wolves (üê∫): Noise points  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Two main hyperparameters**\n",
    "- `eps`: determines what it means for points to be \"close\"\n",
    "- `min_samples`: determines the number of **neighboring points** we require to consider in order for a point to be part of a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Effect of `eps` hyperparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(random_state=40, centers=2, n_samples=6)\n",
    "#interactive(lambda eps=1: plot_dbscan_with_labels(X, eps), eps=(1, 12, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "from panel import widgets\n",
    "from panel.interact import interact\n",
    "import matplotlib\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "def f(eps):\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    return plot_dbscan_with_labels(X, fig, eps)\n",
    "\n",
    "#interact(f, eps=widgets.FloatSlider(start=1, end=12, step=1, value=1))\n",
    "interact(f, eps=widgets.FloatSlider(start=1, end=15, step=2, value=1)).embed(max_opts=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = euclidean_distances(X, X)\n",
    "pd.DataFrame(dists).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists.min(), dists.max(), dists.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Effect of `min_samples` hyperparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(min_samples):\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    return plot_dbscan_with_labels(X, fig, eps=2.0, min_samples=min_samples)\n",
    "\n",
    "interact(f, min_samples=widgets.FloatSlider(start=1, end=5, step=1, value=1))\n",
    "#interact(f, min_samples=widgets.FloatSlider(start=1, end=5, step=1, value=1)).embed(max_opts=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More details on DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Illustration of hyperparameters `eps` and `min_samples`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(random_state=0, n_samples=12)\n",
    "discrete_scatter(X[:, 0], X[:, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(X)\n",
    "print(\"Cluster memberships:{}\".format(clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Default values for hyperparameters don't work well on toy datasets. \n",
    "- All points have been marked as noise with the default values for `eps` and `min_samples`\n",
    "- Let's examine the effect of changing these hyperparameters. \n",
    "    - noise points: shown in white\n",
    "    - core points: bigger\n",
    "    - border points: smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_dbscan()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Observations**\n",
    "\n",
    "- Increasing `eps` ($\\uparrow$) (left to right in the plot above) means more points will be included in a cluster. \n",
    "    - `eps` = 1.0 either creates more clusters or more noise points, whereas eps=3.0 puts all points in one cluster with no noise points.  \n",
    "- Increasing `min_samples` ($\\uparrow$) (top to bottom in the plot above) means points in less dense regions will either be labeled as their own cluster or noise. \n",
    "    - `min_samples=2`, for instance, has none or only a fewer noise points whereas `min_samples=5` has several noise points. \n",
    "- Here `min_samples` = 2.0 or 3.0 and `eps` = 1.5 is giving us the best results. \n",
    "- In general, it's not trivial to tune these hyperparameters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**K-Means vs. DBSCAN**\n",
    "\n",
    "- In DBSCAN, you do not have to specify the number of clusters! \n",
    "    - Instead, you have to tune `eps` and `min_samples`. \n",
    "- Unlike K-Means, DBSCAN doesn't have to assign all points to clusters. \n",
    "    - The label is -1 if a point is unassigned.\n",
    "- Unlike K-Means, there is no `predict` method. \n",
    "    - DBSCAN only really clusters the points you have, not \"new\" or \"test\" points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question for you\n",
    "\n",
    "- Does the order that you pick the points matter in DBSCAN?\n",
    "<br><br><br><br>\n",
    "\n",
    "> No. Any of the cluster's core points is able to fully identify the cluster, with no randomness involved. The only possible conflict you might get is that if two clusters have the same border point. In this case the assignment will be implementation dependent, but usually the border point will be assigned to the first cluster that \"finds\" it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating DBSCAN clusters \n",
    "- We cannot use the elbow method to examine the goodness of clusters created with DBSCAN. \n",
    "- But we can use the silhouette method because it's not dependent on the idea of cluster centers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(random_state=100, centers=3, n_samples=300)\n",
    "dbscan = DBSCAN(eps=2, min_samples=5)\n",
    "dbscan.fit(X)\n",
    "plot_original_clustered(X, dbscan, dbscan.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Yellowbrick is designed to work with K-Means and not with DBSCAN.\n",
    "# So it needs the number of clusters stored in n_clusters\n",
    "# It also needs `predict` method to be implemented.\n",
    "# So I'm implementing it here so that we can use Yellowbrick to show Silhouette plots.\n",
    "n_clusters = len(set(dbscan.labels_))\n",
    "dbscan.n_clusters = n_clusters\n",
    "dbscan.predict = lambda x: dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = SilhouetteVisualizer(dbscan, colors=\"yellowbrick\")\n",
    "visualizer.fit(X)  # Fit the data to the visualizer\n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary: Pros and cons\n",
    "\n",
    "- Pros\n",
    "    - Can learn arbitrary cluster shapes\n",
    "    - Can detect outliers \n",
    "- Cons\n",
    "    - Cannot `predict` on new examples.  \n",
    "    - Needs tuning of two non-obvious hyperparameters \n",
    "\n",
    "There is an improved version of DBSCAN called [`HDBSCAN` (hierarchical DBSCAN)](https://github.com/scikit-learn-contrib/hdbscan). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DBSCAN: failure cases\n",
    "\n",
    "- DBSCAN is able to capture complex clusters. But this doesn't mean that `DBSCAN` always works better. It has its own problems! \n",
    "- DBSCAN doesn't do well when we have clusters with different densities. \n",
    "    - You can play with the hyperparameters but it's not likely to help much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DBSCAN: failure cases\n",
    "\n",
    "- Let's consider this dataset with three clusters of varying densities.  \n",
    "- K-Means performs better compared to DBSCAN. But it has the benefit of knowing the value of $K$ in advance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_varied, y_varied = make_blobs(\n",
    "    n_samples=200, cluster_std=[1.0, 5.0, 1.0], random_state=10\n",
    ")\n",
    "plot_k_means_dbscan_comparison(X_varied)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ‚ùì‚ùì Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 16.2 Select all of the following statements which are **True** (iClicker)\n",
    "\n",
    "- (A) With tiny epsilon (`eps` in `sklearn`) and min samples=1 (`min_samples=1` in `sklearn`) we are likely to end up with each point in its own cluster. \n",
    "- (B) With a smaller value of `eps` and larger number for `min_samples` we are likely to end up with a one big cluster. \n",
    "- (C) K-Means is more susceptible to outliers compared to DBSCAN.  \n",
    "- (D) In DBSCAN to be part of a cluster, each point must have at least `min_samples` neighbours in a given radius (including itself). \n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical clustering [[video](https://www.youtube.com/watch?v=NM8lFKFZ2IU)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introduction \n",
    "- Deciding how many clusters we want is a hard problem. \n",
    "- Often, it's useful to get a complete picture of similarity between points in our data before picking the number of clusters.  \n",
    "- Hierarchical clustering is helpful in these scenarios.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Main idea**\n",
    "\n",
    "1. Start with each point in its own cluster. \n",
    "2. Greedily merge most similar *clusters*. \n",
    "3. Repeat Step 2 until you obtain only one cluster ($n-1$ times)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Hierarchical clustering can be visualized using a tool called **a dendrogram**. \n",
    "- Unfortunately, `sklearn` cannot do it so we will use the package `scipy.cluster.hierarchy` for hierarchical clustering. Let's try it out before understanding how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import (\n",
    "    average,\n",
    "    complete,\n",
    "    dendrogram,\n",
    "    fcluster,\n",
    "    single,\n",
    "    ward,\n",
    ")\n",
    "\n",
    "X_orig, y = make_blobs(random_state=0, n_samples=11)\n",
    "X = StandardScaler().fit_transform(X_orig)\n",
    "linkage_array = ward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_X_dendrogram(X, linkage_array, label_n_clusters=True) # user-defined function defined in code/plotting_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Every point goes through the journey of being on its own (its own cluster) and getting merged with some other bigger clusters. \n",
    "- The intermediate steps in the process provide us clustering with different number of clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dendrogram\n",
    "\n",
    "- Dendrogram is a tree-like plot. \n",
    "- On the x-axis we have data points. \n",
    "- On the y-axis we have distances between clusters. \n",
    "- We start with data points as leaves of the tree.  \n",
    "- New parent node is created for every two clusters that are joined. \n",
    "- The length of each branch shows how far the merged clusters go. \n",
    "    - In the dendrogram above going from three clusters to two clusters means merging far apart points because the branches between three cluster to two clusters are long. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to plot a dendrogram?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "ax = plt.gca()\n",
    "dendrogram(linkage_array, ax=ax, color_threshold=3)\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Cluster distance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note} \n",
    "The colours you see above are based on the `color_threshold` parameter. It colours all the descendent links below a cluster node $k$ the same color if $k$ is the first node below the cut threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Flat clusters**\n",
    "\n",
    "- This is good but how can we get cluster labels from a dendrogram? \n",
    "- We can bring the clustering to a \"flat\" format use `fcluster`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "# flattening the dendrogram based on maximum number of clusters. \n",
    "hier_labels1 = fcluster(linkage_array, 3, criterion=\"maxclust\") \n",
    "hier_labels1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendrogram_clusters(X, linkage_array, hier_labels1, title=\"flattened with max_clusts=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattening the dendrogram based on maximum distance between points. \n",
    "hier_labels2 = fcluster(linkage_array, 0.7, criterion=\"distance\") \n",
    "hier_labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendrogram_clusters(X, linkage_array, hier_labels2, title=\"flattened with dist=0.7\", color_threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When we create a dendrogram, we need to calculate distance between clusters. \n",
    "\n",
    "- We know how to measure distance between points (e.g., using Euclidean distance). \n",
    "- How do we measure distances between clusters? \n",
    "- The **linkage criteria** determines how to find similarity between clusters:\n",
    "- Some example linkage criteria are: \n",
    "    - Single linkage $\\rightarrow$ smallest minimal distance, leads to loose clusters\n",
    "    - Complete linkage $\\rightarrow$ smallest maximum distance, leads to tight clusters \n",
    "    - Average linkage $\\rightarrow$ smallest average distance between all pairs of points in the clusters\n",
    "    - Ward linkage $\\rightarrow$ smallest increase in within-cluster variance, leads to equally sized clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_X_dendrogram(X, linkage_array, label_n_clusters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**`single` linkage**\n",
    "\n",
    "- Merges two clusters that have the smallest minimum distance between all their points. \n",
    "- Let's use `scipy.cluster.hierarchy`'s `single` to get linkage information. \n",
    "- This method gives us matrix `Z` with the merging information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Z = single(X)\n",
    "columns = [\"c1\", \"c2\", \"distance(c1, c2)\", \"# observations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(Z, columns=columns).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- The linkage returns a matrix `Z` of shape n-1 by 4:\n",
    "- The rows represent iterations. \n",
    "- First and second columns (c1 and c2 above): indexes of the clusters being merged.\n",
    "- Third column (distance(c1, c2)): the distance between the clusters being merged.\n",
    "- Fourth column (# observations): the number of examples in the newly formed cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Creating dendrogram with `single` linkage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_array = single(X)\n",
    "hier_labels = fcluster(linkage_array, 3, criterion=\"maxclust\") \n",
    "plot_dendrogram_clusters(X, linkage_array, hier_labels, title=\"maxclust 3\", color_threshold=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we decide to go from 3 clusters to 2 clusters. Which clusters would be merged with single linkage criterion? It will merge the clusters with Suppose we decide to go from 3 clusters to 2 clusters. Which clusters would be merged with single linkage criterion? It will merge the clusters with the smallest minimal distance. the smallest minimal distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_labels = fcluster(linkage_array, 2, criterion=\"maxclust\") \n",
    "plot_dendrogram_clusters(X, linkage_array, hier_labels,title=\"maxclust 2\", color_threshold=1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**`complete` linkage**\n",
    "\n",
    "- Merges two clusters that have the smallest maximum distance between their points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_array = complete(X)\n",
    "hier_labels = fcluster(linkage_array, 3, criterion=\"maxclust\") \n",
    "plot_dendrogram_clusters(X, linkage_array, hier_labels, linkage_type='complete', title=\"maxclust 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we decide to go from 3 clusters to 2 clusters. Which clusters would be merged with single linkage criterion? It will merge the clusters with the smallest maximum distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_labels = fcluster(linkage_array, 2, criterion=\"maxclust\") \n",
    "plot_dendrogram_clusters(X, linkage_array, hier_labels, linkage_type='complete', title=\"maxclust 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**`average` linkage**\n",
    "\n",
    "- Merges two clusters that have the smallest average distance between all their points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_array = average(X)\n",
    "hier_labels = fcluster(linkage_array, 3, criterion=\"maxclust\") \n",
    "plot_dendrogram_clusters(X, linkage_array, hier_labels, title=\"maxclust 3\", color_threshold=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we decide to go from 3 clusters to 2 clusters. Which clusters would be merged with single linkage criterion? It will merge the clusters with the smallest average distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_labels = fcluster(linkage_array, 2, criterion=\"maxclust\") \n",
    "plot_dendrogram_clusters(X, linkage_array, hier_labels, linkage_type='average', title=\"maxclust 2\", color_threshold=2.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**`ward` linkage**\n",
    "\n",
    "- Picks two clusters to merge such that the variance within all clusters increases the least. \n",
    "- Often leads to equally sized clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_array = ward(X)\n",
    "hier_labels = fcluster(linkage_array, 3, criterion=\"maxclust\") \n",
    "plot_dendrogram_clusters(X, linkage_array, hier_labels, linkage_type='ward', title=\"maxclust 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we decide to go from 3 clusters to 2 clusters. Which clusters would be merged with ward linkage criterion? It will merge the clusters with the smallest within-cluster variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_labels = fcluster(linkage_array, 2, criterion=\"maxclust\") \n",
    "plot_dendrogram_clusters(X, linkage_array, hier_labels, linkage_type='ward', title=\"maxclust 2\", color_threshold=4.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different type of linkage criterion can be appropriate in different scenarios and the goals of clustering analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X2, y = make_blobs(n_samples=200, centers=3, random_state=0)\n",
    "np.random.seed(42)\n",
    "X2[:50] += np.random.uniform(low=0.3, high=1, size=(50, 2)) # Add some noise to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linkage_criteria(X2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Single linkage $\\rightarrow$ smallest minimal distance, leads to loose clusters\n",
    "- Complete linkage $\\rightarrow$ smallest maximum distance, leads to tight clusters \n",
    "- Average linkage $\\rightarrow$ smallest average distance between all pairs of points in the clusters\n",
    "- Ward linkage $\\rightarrow$ smallest increase in within-cluster variance, leads to equally sized clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Truncation**\n",
    "\n",
    "This is a toy dataset and it's possible to view all the leaves. But what if we have thousands of examples, which is common in real-world datasets? We can use truncation. There are two levels of truncation supported in `scipy.cluster.hierarchy`. \n",
    "- `level` $\\rightarrow$ Maximum depth of the tree is $p$\n",
    "- `lastp` $\\rightarrow$ Only $p$ leaves are shown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = single(X)\n",
    "dendrogram(Z, p=2, truncate_mode=\"level\");\n",
    "# p is the max depth of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram(Z, p=5, truncate_mode=\"lastp\");\n",
    "# p is the number of leaf nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_truncation_toy_demo(Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```note\n",
    "I‚Äôm not quite sure why we see 0 and 2 not merged for truncation_mode = 'level' and p=1 ü§î.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ‚ùì‚ùì Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 16.3 Select all of the following statements which are **True** \n",
    "\n",
    "- (A) In hierarchical clustering we do not have to worry about initialization. \n",
    "- (B) Hierarchical clustering can only be applied to smaller datasets because dendrograms are hard to visualize for large datasets.\n",
    "- (C) In all the clustering methods we have seen (K-Means, DBSCAN, hierarchical clustering), there is a way to decide the granularity of clustering (i.e., how many clusters to pick). \n",
    "- (D) To get robust clustering we can naively ensemble cluster labels (e.g., pick the most popular label) produced by different clustering methods. \n",
    "- (E) If you have a high Silhouette score and very clean and robust clusters, it means that the algorithm has captured the semantic meaning in the data of our interest.   \n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final comments, summary, and reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Take-home message\n",
    "\n",
    "- We saw four methods for clustering: Centroid-based clustering (K-Means), distribution-based clustering (Gaussian mixture models), density-based clustering (DBSCAN), and hierarchical clustering. \n",
    "- There are many more clustering algorithms out there which we didn't talk about. For example see [this overview of clustering methods](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods). \n",
    "- Two important aspects of clustering\n",
    "    - Choice of distance metric\n",
    "    - Data representation\n",
    "- Choosing the appropriate number of clusters for a given problem is quite hard. \n",
    "- A lot of manual interpretation is involved in clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A few comments on clustering evaluation\n",
    "\n",
    "- If you know the ground truth, you can use metrics such as [adjusted random score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html) or [normalized mutual information score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html). \n",
    "- We can't use accuracy scores. \n",
    "    - Because the labels themselves are meaningless in clustering.  \n",
    "- Usually ground truth is not available, and if it is available we would probably go with supervised models.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- The silhouette score works for different clustering methods and it can give us some intuition about the quality of clusters. But it's not very interpretable on real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A couple of ways to evaluate clustering: \n",
    "- Using *robustness-based* clustering metrics\n",
    "- The idea is to run a clustering algorithm or a number of clustering algorithms after adding some noise to the data or using different parameter settings and comparing outcomes. \n",
    "- If many models, perturbations, and parameters are giving the same result, the clustering is likely to be trustworthy.  \n",
    "- But how do we compare clustering partitions of two different clustering models? We can use some metrics which are based on the [contingency matrix](http://www.comparingpartitions.info/?link=Tut14).  \n",
    "- That said, even though all clustering models give similar results, the clusters might not capture the aspect you are interested in. \n",
    "- So you cannot really avoid manual inspection. \n",
    "- Manual inspection and analysis is tedious but remember that this is what makes data scientists valuable. We can focus on the things ML models are not good at. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources \n",
    "\n",
    "- Check out this nice comparison of [sklearn clustering algorithms](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods).\n",
    "- [DBSCAN Visualization](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)\n",
    "- There is also [Hierarchical DBSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html). \n",
    "- [Clustering with Scikit with GIFs](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
